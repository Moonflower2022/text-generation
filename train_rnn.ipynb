{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "from torch import nn # network cell, for LSTM\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of characters: 254514\n",
      "# of unique characters (INPUT_SIZE): 76\n"
     ]
    }
   ],
   "source": [
    "text_name = \"peter_pan\"\n",
    "\n",
    "with open(f\"texts/{text_name}.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"# of characters:\", len(text))\n",
    "\n",
    "unique_characters = set(text)\n",
    "INPUT_SIZE = len(unique_characters)\n",
    "print(\"# of unique characters (INPUT_SIZE):\", INPUT_SIZE)\n",
    "\n",
    "ordered_characters = sorted(unique_characters)\n",
    "\n",
    "CHARACTER_ENCODING = dict(zip(ordered_characters, list(range(len(ordered_characters)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(character): # one hot\n",
    "    encoding = torch.zeros(INPUT_SIZE)\n",
    "    encoding[CHARACTER_ENCODING[character]] = 1\n",
    "    return encoding\n",
    "\n",
    "def encode_string(string):\n",
    "    encoding = torch.zeros(len(string), INPUT_SIZE)\n",
    "    for i in range(len(string)):\n",
    "        encoding[i] = encode_char(string[i])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([254438, 10, 76])\n",
      "y shape: torch.Size([254438, 76])\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "INPUT_SEQUENCE_LENGTH = 10\n",
    "\n",
    "for i in range(len(text) - INPUT_SIZE):\n",
    "    # The input sequence\n",
    "    sequence = encode_string(text[i: i + INPUT_SEQUENCE_LENGTH])\n",
    "    # The next character (one-hot encoded) as label\n",
    "    next_character = encode_char(text[i + INPUT_SEQUENCE_LENGTH])    \n",
    "\n",
    "    X.append(sequence)\n",
    "    y.append(next_character)\n",
    "\n",
    "X = torch.stack(X)  # Shape: (num_samples, sequence_length, INPUT_SIZE)\n",
    "y = torch.stack(y)  # Shape: (num_samples, INPUT_SIZE)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 76])\n"
     ]
    }
   ],
   "source": [
    "class GRUCharPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUCharPredictor, self).__init__()\n",
    "        self.lstm = nn.GRU(input_size=INPUT_SIZE, hidden_size=256, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256 * 2, INPUT_SIZE)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        linear_out = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return linear_out\n",
    "\n",
    "class LSTMCharPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMCharPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=INPUT_SIZE, hidden_size=256, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256 * 2, INPUT_SIZE)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        linear_out = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return linear_out\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMCharPredictor().to(device)\n",
    "\n",
    "print(model(X[0].unsqueeze(0).to(device)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 2.0310022830963135\n",
      "Epoch [2/30], Loss: 1.4168167114257812\n",
      "Epoch [3/30], Loss: 1.241541862487793\n",
      "Epoch [4/30], Loss: 1.3444234132766724\n",
      "Epoch [5/30], Loss: 1.438924789428711\n",
      "Epoch [6/30], Loss: 0.9437075257301331\n",
      "Epoch [7/30], Loss: 1.2759875059127808\n",
      "Epoch [8/30], Loss: 1.067692756652832\n",
      "Epoch [9/30], Loss: 1.0428367853164673\n",
      "Epoch [10/30], Loss: 1.51625657081604\n",
      "Epoch [11/30], Loss: 0.721851646900177\n",
      "Epoch [12/30], Loss: 0.9103653430938721\n",
      "Epoch [13/30], Loss: 0.8300895690917969\n",
      "Epoch [14/30], Loss: 1.3749632835388184\n",
      "Epoch [15/30], Loss: 0.8174614906311035\n",
      "Epoch [16/30], Loss: 0.9576235413551331\n",
      "Epoch [17/30], Loss: 0.7835237383842468\n",
      "Epoch [18/30], Loss: 0.8132526278495789\n",
      "Epoch [19/30], Loss: 0.7795225381851196\n",
      "Epoch [20/30], Loss: 0.8173518180847168\n",
      "Epoch [21/30], Loss: 0.8097052574157715\n",
      "Epoch [22/30], Loss: 0.4254661798477173\n",
      "Epoch [23/30], Loss: 0.36994320154190063\n",
      "Epoch [24/30], Loss: 0.6652113795280457\n",
      "Epoch [25/30], Loss: 0.5546661615371704\n",
      "Epoch [26/30], Loss: 0.6775380969047546\n",
      "Epoch [27/30], Loss: 0.5883356332778931\n",
      "Epoch [28/30], Loss: 0.5796442627906799\n",
      "Epoch [29/30], Loss: 0.4102676212787628\n",
      "Epoch [30/30], Loss: 0.5887817144393921\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X.size(0))\n",
    "    \n",
    "    for i in range(0, X.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = X[indices].to(device), y[indices].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        labels = torch.argmax(batch_y, dim=1)  # Convert one-hot to class indices\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '(', 4: ')', 5: ',', 6: '-', 7: '.', 8: '0', 9: '1', 10: '2', 11: '3', 12: '4', 13: '6', 14: '7', 15: ':', 16: ';', 17: '?', 18: 'A', 19: 'B', 20: 'C', 21: 'D', 22: 'E', 23: 'F', 24: 'G', 25: 'H', 26: 'I', 27: 'J', 28: 'K', 29: 'L', 30: 'M', 31: 'N', 32: 'O', 33: 'P', 34: 'Q', 35: 'R', 36: 'S', 37: 'T', 38: 'U', 39: 'V', 40: 'W', 41: 'X', 42: 'Y', 43: 'Z', 44: 'a', 45: 'b', 46: 'c', 47: 'd', 48: 'e', 49: 'f', 50: 'g', 51: 'h', 52: 'i', 53: 'j', 54: 'k', 55: 'l', 56: 'm', 57: 'n', 58: 'o', 59: 'p', 60: 'q', 61: 'r', 62: 's', 63: 't', 64: 'u', 65: 'v', 66: 'w', 67: 'x', 68: 'y', 69: 'z', 70: 'é', 71: '—', 72: '‘', 73: '’', 74: '“', 75: '”'}\n",
      "{'\\n': 0, ' ': 1, '!': 2, '(': 3, ')': 4, ',': 5, '-': 6, '.': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '6': 13, '7': 14, ':': 15, ';': 16, '?': 17, 'A': 18, 'B': 19, 'C': 20, 'D': 21, 'E': 22, 'F': 23, 'G': 24, 'H': 25, 'I': 26, 'J': 27, 'K': 28, 'L': 29, 'M': 30, 'N': 31, 'O': 32, 'P': 33, 'Q': 34, 'R': 35, 'S': 36, 'T': 37, 'U': 38, 'V': 39, 'W': 40, 'X': 41, 'Y': 42, 'Z': 43, 'a': 44, 'b': 45, 'c': 46, 'd': 47, 'e': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'p': 59, 'q': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, 'v': 65, 'w': 66, 'x': 67, 'y': 68, 'z': 69, 'é': 70, '—': 71, '‘': 72, '’': 73, '“': 74, '”': 75}\n"
     ]
    }
   ],
   "source": [
    "INDEX_ENCODING = {}\n",
    "\n",
    "for char, i in CHARACTER_ENCODING.items():\n",
    "    INDEX_ENCODING[i] = char\n",
    "\n",
    "print(INDEX_ENCODING)\n",
    "print(CHARACTER_ENCODING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sequence(sequence, deterministic=False):\n",
    "    logits = model(\n",
    "        encode_string(sequence[-INPUT_SEQUENCE_LENGTH:]).unsqueeze(0).to(device)\n",
    "    ) # returns in size (1, INPUT_SEQUENCE_LENGTH, 78)\n",
    "\n",
    "    probabilities = torch.softmax(logits[0], dim=1)\n",
    "\n",
    "    character_indexes = torch.argmax(probabilities) if deterministic else torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    next_sequence = [INDEX_ENCODING[int(index)] for index in character_indexes]\n",
    "    \n",
    "    return next_sequence\n",
    "\n",
    "def get_next_char(sequence, deterministic=False):\n",
    "    logits = model(\n",
    "        encode_string(sequence[-INPUT_SEQUENCE_LENGTH:]).unsqueeze(0).to(device)\n",
    "    ) # returns in size (1, 78)\n",
    "\n",
    "    probabilities = torch.softmax(logits[0], 0)\n",
    "\n",
    "    character_index = torch.argmax(probabilities) if deterministic else torch.multinomial(probabilities, num_samples=1)\n",
    "    \n",
    "    return INDEX_ENCODING[int(character_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n"
     ]
    }
   ],
   "source": [
    "sequence = \"P\"\n",
    "print(get_next_char(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(starting_character, num_generated_characters):\n",
    "    generated_text = starting_character\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generated_characters):\n",
    "            generated_text += get_next_char(generated_text)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter. In the chemselves. He must be Peter to Smee an understand ever felt for the Neverland had gone from the catle over the lagoon. He thinned by Peter.\n",
      "\n",
      "In his dagger upraised him one of the recent cast from his crew. A man, rat, and Tink noted, alone seen them.\n",
      "\n",
      "Wendy immensely. “I think, you know. But he reserted, holding Nana’s abrack and fetch meronitain, as soon as so bin that they sired side her up in the drawing-room; and yes, he who had lost faithed him on the ground by this probably the trees. They said “How ripping,” but did not.\n",
      "\n",
      "“Oh, the cleverness; but standing erect, but it sounding ones are surescented with such an arms of scortly.\n",
      "\n",
      "O Wendy?”\n",
      "\n",
      "“I forget dell from their trangles table, who had gone into the office again, we fround only one of us had a mother.”\n",
      "\n",
      "They are the children call never heard. The others would not let her go.”\n",
      "\n",
      "“It is just thinking,” he said, almost blew he thought the sound of the ticks he bid.”\n",
      "\n",
      "“What kind of afraid of the greatest danger, only\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(\"P\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_avaialable_file_name(file_name, extension):\n",
    "    available_file_name = file_name\n",
    "    i = 1\n",
    "    while os.path.isfile(available_file_name + extension):\n",
    "        available_file_name = file_name + f\"_{i}\"\n",
    "        i += 1\n",
    "\n",
    "    return available_file_name + extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    get_avaialable_file_name(\n",
    "        f\"generated_texts/{text_name}_gru_{INPUT_SEQUENCE_LENGTH}chars\", \".txt\"\n",
    "    ),\n",
    "    \"w\",\n",
    ") as file:\n",
    "    file.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    get_avaialable_file_name(\n",
    "        f\"models/{text_name}_lstm_{INPUT_SEQUENCE_LENGTH}chars\", \".pth\"\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
