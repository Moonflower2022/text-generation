{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "from torch import nn # network cell, for LSTM\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of characters: 667482\n",
      "# of unique characters (INPUT_SIZE): 151\n"
     ]
    }
   ],
   "source": [
    "text_name = \"python_snippets_1000\"\n",
    "\n",
    "with open(f\"texts/{text_name}.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"# of characters:\", len(text))\n",
    "\n",
    "unique_characters = set(text)\n",
    "INPUT_SIZE = len(unique_characters)\n",
    "print(\"# of unique characters (INPUT_SIZE):\", INPUT_SIZE)\n",
    "\n",
    "ordered_characters = sorted(unique_characters)\n",
    "\n",
    "CHARACTER_ENCODING = dict(zip(ordered_characters, list(range(len(ordered_characters)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(character): # one hot\n",
    "    encoding = torch.zeros(INPUT_SIZE)\n",
    "    encoding[CHARACTER_ENCODING[character]] = 1\n",
    "    return encoding\n",
    "\n",
    "def encode_string(string):\n",
    "    encoding = torch.zeros(len(string), INPUT_SIZE)\n",
    "    for i in range(len(string)):\n",
    "        encoding[i] = encode_char(string[i])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([667331, 10, 151])\n",
      "y shape: torch.Size([667331, 151])\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "INPUT_SEQUENCE_LENGTH = 10\n",
    "\n",
    "for i in range(len(text) - INPUT_SIZE):\n",
    "    # The input sequence\n",
    "    sequence = encode_string(text[i: i + INPUT_SEQUENCE_LENGTH])\n",
    "    # The next character (one-hot encoded) as label\n",
    "    next_character = encode_char(text[i + INPUT_SEQUENCE_LENGTH])    \n",
    "\n",
    "    X.append(sequence)\n",
    "    y.append(next_character)\n",
    "\n",
    "X = torch.stack(X)  # Shape: (num_samples, sequence_length, INPUT_SIZE)\n",
    "y = torch.stack(y)  # Shape: (num_samples, INPUT_SIZE)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 151])\n"
     ]
    }
   ],
   "source": [
    "class GRUCharPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUCharPredictor, self).__init__()\n",
    "        self.lstm = nn.GRU(input_size=INPUT_SIZE, hidden_size=256, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256 * 2, INPUT_SIZE)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        linear_out = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return linear_out\n",
    "\n",
    "class LSTMCharPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMCharPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=INPUT_SIZE, hidden_size=256, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256 * 2, INPUT_SIZE)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        linear_out = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return linear_out\n",
    "\n",
    "model_type = \"lstm\"\n",
    "\n",
    "# Initialize the model\n",
    "if model_type == \"lstm\":\n",
    "    model = LSTMCharPredictor().to(device)\n",
    "elif model_type == \"gru\":\n",
    "    model = GRUCharPredictor().to(device)\n",
    "\n",
    "print(model(X[0].unsqueeze(0).to(device)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.4349181652069092\n",
      "Epoch [2/30], Loss: 0.813270628452301\n",
      "Epoch [3/30], Loss: 0.9003188610076904\n",
      "Epoch [4/30], Loss: 0.8389748334884644\n",
      "Epoch [5/30], Loss: 1.2761476039886475\n",
      "Epoch [6/30], Loss: 1.4571102857589722\n",
      "Epoch [7/30], Loss: 0.8303962349891663\n",
      "Epoch [8/30], Loss: 0.5914250016212463\n",
      "Epoch [9/30], Loss: 0.8748084902763367\n",
      "Epoch [10/30], Loss: 0.6436059474945068\n",
      "Epoch [11/30], Loss: 0.5765541195869446\n",
      "Epoch [12/30], Loss: 0.7960420250892639\n",
      "Epoch [13/30], Loss: 0.9703429937362671\n",
      "Epoch [14/30], Loss: 0.39227357506752014\n",
      "Epoch [15/30], Loss: 0.6770938634872437\n",
      "Epoch [16/30], Loss: 1.0812124013900757\n",
      "Epoch [17/30], Loss: 0.628760039806366\n",
      "Epoch [18/30], Loss: 0.1532524973154068\n",
      "Epoch [19/30], Loss: 0.30108094215393066\n",
      "Epoch [20/30], Loss: 0.43620914220809937\n",
      "Epoch [21/30], Loss: 0.6956696510314941\n",
      "Epoch [22/30], Loss: 0.5405467748641968\n",
      "Epoch [23/30], Loss: 0.2519914209842682\n",
      "Epoch [24/30], Loss: 0.6852290630340576\n",
      "Epoch [25/30], Loss: 0.6391257643699646\n",
      "Epoch [26/30], Loss: 0.3332698941230774\n",
      "Epoch [27/30], Loss: 0.0933278277516365\n",
      "Epoch [28/30], Loss: 0.9615123271942139\n",
      "Epoch [29/30], Loss: 0.650242030620575\n",
      "Epoch [30/30], Loss: 0.5267406105995178\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X.size(0))\n",
    "    \n",
    "    for i in range(0, X.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = X[indices].to(device), y[indices].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        labels = torch.argmax(batch_y, dim=1)  # Convert one-hot to class indices\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '#', 5: '$', 6: '%', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: '*', 12: '+', 13: ',', 14: '-', 15: '.', 16: '/', 17: '0', 18: '1', 19: '2', 20: '3', 21: '4', 22: '5', 23: '6', 24: '7', 25: '8', 26: '9', 27: ':', 28: ';', 29: '<', 30: '=', 31: '>', 32: '?', 33: '@', 34: 'A', 35: 'B', 36: 'C', 37: 'D', 38: 'E', 39: 'F', 40: 'G', 41: 'H', 42: 'I', 43: 'J', 44: 'K', 45: 'L', 46: 'M', 47: 'N', 48: 'O', 49: 'P', 50: 'Q', 51: 'R', 52: 'S', 53: 'T', 54: 'U', 55: 'V', 56: 'W', 57: 'X', 58: 'Y', 59: 'Z', 60: '[', 61: '\\\\', 62: ']', 63: '^', 64: '_', 65: '`', 66: 'a', 67: 'b', 68: 'c', 69: 'd', 70: 'e', 71: 'f', 72: 'g', 73: 'h', 74: 'i', 75: 'j', 76: 'k', 77: 'l', 78: 'm', 79: 'n', 80: 'o', 81: 'p', 82: 'q', 83: 'r', 84: 's', 85: 't', 86: 'u', 87: 'v', 88: 'w', 89: 'x', 90: 'y', 91: 'z', 92: '{', 93: '|', 94: '}', 95: '~', 96: '¬∞', 97: '¬≤', 98: '√°', 99: '√ß', 100: '√©', 101: '√±', 102: 'Í∞Ä', 103: 'Í∞Å', 104: 'Í∞ï', 105: 'Í±¥', 106: 'Í≤Ω', 107: 'Í≥º', 108: 'Í∏∞', 109: 'Îäî', 110: 'Îä•', 111: 'ÎèÑ', 112: 'Îê®', 113: 'Îì†', 114: 'Î†§', 115: 'Î†•', 116: 'Î°ú', 117: 'Îßå', 118: 'Î©¥', 119: 'Î™®', 120: 'Î™©', 121: 'ÏÉù', 122: 'ÏÑ†', 123: 'ÏÑ±', 124: 'Ïàò', 125: 'ÏïÑ', 126: 'Ïïä', 127: 'Ïïº', 128: 'ÏóÖ', 129: 'ÏóÜ', 130: 'Ïó¨', 131: 'Ïö∞', 132: 'Ïúº', 133: 'ÏùÑ', 134: 'Ïùå', 135: 'Ïùò', 136: 'Ï°∞', 137: 'Ï°±', 138: 'ÏßÄ', 139: 'Ï∞æ', 140: 'Ï∂ú', 141: 'ÌÅ¨', 142: 'Ìëú', 143: 'Ìïò', 144: 'Ìïú', 145: 'Ìï†', 146: 'Ìï®', 147: 'Ìï©', 148: 'Ìï¥', 149: 'ÌòÑ', 150: 'üòä'}\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, ';': 28, '<': 29, '=': 30, '>': 31, '?': 32, '@': 33, 'A': 34, 'B': 35, 'C': 36, 'D': 37, 'E': 38, 'F': 39, 'G': 40, 'H': 41, 'I': 42, 'J': 43, 'K': 44, 'L': 45, 'M': 46, 'N': 47, 'O': 48, 'P': 49, 'Q': 50, 'R': 51, 'S': 52, 'T': 53, 'U': 54, 'V': 55, 'W': 56, 'X': 57, 'Y': 58, 'Z': 59, '[': 60, '\\\\': 61, ']': 62, '^': 63, '_': 64, '`': 65, 'a': 66, 'b': 67, 'c': 68, 'd': 69, 'e': 70, 'f': 71, 'g': 72, 'h': 73, 'i': 74, 'j': 75, 'k': 76, 'l': 77, 'm': 78, 'n': 79, 'o': 80, 'p': 81, 'q': 82, 'r': 83, 's': 84, 't': 85, 'u': 86, 'v': 87, 'w': 88, 'x': 89, 'y': 90, 'z': 91, '{': 92, '|': 93, '}': 94, '~': 95, '¬∞': 96, '¬≤': 97, '√°': 98, '√ß': 99, '√©': 100, '√±': 101, 'Í∞Ä': 102, 'Í∞Å': 103, 'Í∞ï': 104, 'Í±¥': 105, 'Í≤Ω': 106, 'Í≥º': 107, 'Í∏∞': 108, 'Îäî': 109, 'Îä•': 110, 'ÎèÑ': 111, 'Îê®': 112, 'Îì†': 113, 'Î†§': 114, 'Î†•': 115, 'Î°ú': 116, 'Îßå': 117, 'Î©¥': 118, 'Î™®': 119, 'Î™©': 120, 'ÏÉù': 121, 'ÏÑ†': 122, 'ÏÑ±': 123, 'Ïàò': 124, 'ÏïÑ': 125, 'Ïïä': 126, 'Ïïº': 127, 'ÏóÖ': 128, 'ÏóÜ': 129, 'Ïó¨': 130, 'Ïö∞': 131, 'Ïúº': 132, 'ÏùÑ': 133, 'Ïùå': 134, 'Ïùò': 135, 'Ï°∞': 136, 'Ï°±': 137, 'ÏßÄ': 138, 'Ï∞æ': 139, 'Ï∂ú': 140, 'ÌÅ¨': 141, 'Ìëú': 142, 'Ìïò': 143, 'Ìïú': 144, 'Ìï†': 145, 'Ìï®': 146, 'Ìï©': 147, 'Ìï¥': 148, 'ÌòÑ': 149, 'üòä': 150}\n"
     ]
    }
   ],
   "source": [
    "INDEX_ENCODING = {}\n",
    "\n",
    "for char, i in CHARACTER_ENCODING.items():\n",
    "    INDEX_ENCODING[i] = char\n",
    "\n",
    "print(INDEX_ENCODING)\n",
    "print(CHARACTER_ENCODING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sequence(sequence, deterministic=False):\n",
    "    logits = model(\n",
    "        encode_string(sequence[-INPUT_SEQUENCE_LENGTH:]).unsqueeze(0).to(device)\n",
    "    ) # returns in size (1, INPUT_SEQUENCE_LENGTH, 78)\n",
    "\n",
    "    probabilities = torch.softmax(logits[0], dim=1)\n",
    "\n",
    "    character_indexes = torch.argmax(probabilities) if deterministic else torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    next_sequence = [INDEX_ENCODING[int(index)] for index in character_indexes]\n",
    "    \n",
    "    return next_sequence\n",
    "\n",
    "def get_next_char(sequence, deterministic=False):\n",
    "    logits = model(\n",
    "        encode_string(sequence[-INPUT_SEQUENCE_LENGTH:]).unsqueeze(0).to(device)\n",
    "    ) # returns in size (1, 78)\n",
    "\n",
    "    probabilities = torch.softmax(logits[0], 0)\n",
    "\n",
    "    character_index = torch.argmax(probabilities) if deterministic else torch.multinomial(probabilities, num_samples=1)\n",
    "    \n",
    "    return INDEX_ENCODING[int(character_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "sequence = \"d\"\n",
    "print(get_next_char(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(starting_character, num_generated_characters):\n",
    "    generated_text = starting_character\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generated_characters):\n",
    "            generated_text += get_next_char(generated_text)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 + current_value = {}  # Dictionary to store the most common divisor (gcd) the lamin, required_up(xml_to_x):\n",
      "        num += 1\n",
      "    \n",
      "    total_sum += num\n",
      "        for i in range(2, int(n**0.5) + 1):\n",
      "            return\n",
      "        Print(unix_timestamp = round(g)\n",
      "              return False\n",
      "    for i in range(m + 1)], i + 1):\n",
      "              result += term\n",
      "    return None\n",
      "        attributes\n",
      "        fibonacci_nums, perfect_square(11, \"123 Valul\", result)\n",
      "        exit()\n",
      "\n",
      "# Print the retrieved_time = time.time() - start_time = time.time()\n",
      "\n",
      "        if char in occurrences = count_of_divisible_index\n",
      "           # No match of dary\n",
      "           arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "        file_path = self.prompt_repores.append(i)\n",
      "\n",
      "    return sum = num**3\n",
      "def validate_component(components(component_type\n",
      "        prime_factors(n)\n",
      "             word_list = list(string)\n",
      "        if elem != string2[j-1] == target:\n",
      "        right = len(char_end)\n",
      "        romanNumeral\n",
      "        self.account_naid(self):\n",
      "                  if\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(\"7\", 1000)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_avaialable_file_name(file_name, extension):\n",
    "    available_file_name = file_name\n",
    "    i = 1\n",
    "    while os.path.isfile(available_file_name + extension):\n",
    "        available_file_name = file_name + f\"_{i}\"\n",
    "        i += 1\n",
    "\n",
    "    return available_file_name + extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    get_avaialable_file_name(\n",
    "        f\"generated_texts/{text_name}_{model_type}_{INPUT_SEQUENCE_LENGTH}chars\", \".txt\"\n",
    "    ),\n",
    "    \"w\",\n",
    ") as file:\n",
    "    file.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    get_avaialable_file_name(\n",
    "        f\"models/{text_name}_{model_type}_{INPUT_SEQUENCE_LENGTH}chars\", \".pth\"\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
