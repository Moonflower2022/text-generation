{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "from torch import nn # network cell, for LSTM\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of characters: 254514\n",
      "# of unique characters (INPUT_SIZE): 76\n"
     ]
    }
   ],
   "source": [
    "text_name = \"peter_pan\"\n",
    "\n",
    "with open(f\"texts/{text_name}.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(\"# of characters:\", len(text))\n",
    "\n",
    "unique_characters = set(text)\n",
    "INPUT_SIZE = len(unique_characters)\n",
    "print(\"# of unique characters (INPUT_SIZE):\", INPUT_SIZE)\n",
    "\n",
    "ordered_characters = sorted(unique_characters)\n",
    "\n",
    "CHARACTER_ENCODING = dict(zip(ordered_characters, list(range(len(ordered_characters)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char(character): # one hot\n",
    "    encoding = torch.zeros(INPUT_SIZE)\n",
    "    encoding[CHARACTER_ENCODING[character]] = 1\n",
    "    return encoding\n",
    "\n",
    "def encode_string(string):\n",
    "    encoding = torch.zeros(len(string), INPUT_SIZE)\n",
    "    for i in range(len(string)):\n",
    "        encoding[i] = encode_char(string[i])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([254438, 5, 76])\n",
      "y shape: torch.Size([254438, 76])\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "INPUT_SEQUENCE_LENGTH = 5\n",
    "\n",
    "for i in range(len(text) - INPUT_SIZE):\n",
    "    # The input sequence\n",
    "    sequence = encode_string(text[i: i + INPUT_SEQUENCE_LENGTH])\n",
    "    # The next character (one-hot encoded) as label\n",
    "    next_character = encode_char(text[i + INPUT_SEQUENCE_LENGTH])    \n",
    "\n",
    "    X.append(sequence)\n",
    "    y.append(next_character)\n",
    "\n",
    "X = torch.stack(X)  # Shape: (num_samples, sequence_length, INPUT_SIZE)\n",
    "y = torch.stack(y)  # Shape: (num_samples, INPUT_SIZE)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 76])\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 256  # Number of features in the hidden state\n",
    "NUM_LAYERS = 3  # Number of LSTM layers\n",
    "BATCH_FIRST = True\n",
    "BIDIRECTIONAL = True\n",
    "    \n",
    "class GRUCharPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRUCharPredictor, self).__init__()\n",
    "        self.lstm = nn.GRU(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, batch_first=BATCH_FIRST, bidirectional=BIDIRECTIONAL)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE * (2 if BIDIRECTIONAL else 1), INPUT_SIZE)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        linear_out = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return linear_out\n",
    "\n",
    "# Initialize the model\n",
    "model = GRUCharPredictor().to(device)\n",
    "\n",
    "print(model(X[0].unsqueeze(0).to(device)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.8739911317825317\n",
      "Epoch [2/30], Loss: 1.3424981832504272\n",
      "Epoch [3/30], Loss: 1.287219762802124\n",
      "Epoch [4/30], Loss: 1.2068039178848267\n",
      "Epoch [5/30], Loss: 1.5500156879425049\n",
      "Epoch [6/30], Loss: 1.5336246490478516\n",
      "Epoch [7/30], Loss: 1.4718962907791138\n",
      "Epoch [8/30], Loss: 1.1678557395935059\n",
      "Epoch [9/30], Loss: 1.2409346103668213\n",
      "Epoch [10/30], Loss: 0.7369239330291748\n",
      "Epoch [11/30], Loss: 1.0958571434020996\n",
      "Epoch [12/30], Loss: 1.1929247379302979\n",
      "Epoch [13/30], Loss: 1.7614456415176392\n",
      "Epoch [14/30], Loss: 0.9504497647285461\n",
      "Epoch [15/30], Loss: 0.7592466473579407\n",
      "Epoch [16/30], Loss: 1.325872540473938\n",
      "Epoch [17/30], Loss: 1.2371306419372559\n",
      "Epoch [18/30], Loss: 1.1353330612182617\n",
      "Epoch [19/30], Loss: 0.7843327522277832\n",
      "Epoch [20/30], Loss: 1.272673487663269\n",
      "Epoch [21/30], Loss: 1.0598859786987305\n",
      "Epoch [22/30], Loss: 1.2287068367004395\n",
      "Epoch [23/30], Loss: 1.2360602617263794\n",
      "Epoch [24/30], Loss: 1.2255055904388428\n",
      "Epoch [25/30], Loss: 1.256123661994934\n",
      "Epoch [26/30], Loss: 1.2074202299118042\n",
      "Epoch [27/30], Loss: 0.6831978559494019\n",
      "Epoch [28/30], Loss: 1.105513572692871\n",
      "Epoch [29/30], Loss: 1.0548864603042603\n",
      "Epoch [30/30], Loss: 1.1125582456588745\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X.size(0))\n",
    "    \n",
    "    for i in range(0, X.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = X[indices].to(device), y[indices].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        labels = torch.argmax(batch_y, dim=1)  # Convert one-hot to class indices\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '(', 4: ')', 5: ',', 6: '-', 7: '.', 8: '0', 9: '1', 10: '2', 11: '3', 12: '4', 13: '6', 14: '7', 15: ':', 16: ';', 17: '?', 18: 'A', 19: 'B', 20: 'C', 21: 'D', 22: 'E', 23: 'F', 24: 'G', 25: 'H', 26: 'I', 27: 'J', 28: 'K', 29: 'L', 30: 'M', 31: 'N', 32: 'O', 33: 'P', 34: 'Q', 35: 'R', 36: 'S', 37: 'T', 38: 'U', 39: 'V', 40: 'W', 41: 'X', 42: 'Y', 43: 'Z', 44: 'a', 45: 'b', 46: 'c', 47: 'd', 48: 'e', 49: 'f', 50: 'g', 51: 'h', 52: 'i', 53: 'j', 54: 'k', 55: 'l', 56: 'm', 57: 'n', 58: 'o', 59: 'p', 60: 'q', 61: 'r', 62: 's', 63: 't', 64: 'u', 65: 'v', 66: 'w', 67: 'x', 68: 'y', 69: 'z', 70: 'é', 71: '—', 72: '‘', 73: '’', 74: '“', 75: '”'}\n",
      "{'\\n': 0, ' ': 1, '!': 2, '(': 3, ')': 4, ',': 5, '-': 6, '.': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '6': 13, '7': 14, ':': 15, ';': 16, '?': 17, 'A': 18, 'B': 19, 'C': 20, 'D': 21, 'E': 22, 'F': 23, 'G': 24, 'H': 25, 'I': 26, 'J': 27, 'K': 28, 'L': 29, 'M': 30, 'N': 31, 'O': 32, 'P': 33, 'Q': 34, 'R': 35, 'S': 36, 'T': 37, 'U': 38, 'V': 39, 'W': 40, 'X': 41, 'Y': 42, 'Z': 43, 'a': 44, 'b': 45, 'c': 46, 'd': 47, 'e': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'p': 59, 'q': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, 'v': 65, 'w': 66, 'x': 67, 'y': 68, 'z': 69, 'é': 70, '—': 71, '‘': 72, '’': 73, '“': 74, '”': 75}\n"
     ]
    }
   ],
   "source": [
    "INDEX_ENCODING = {}\n",
    "\n",
    "for char, i in CHARACTER_ENCODING.items():\n",
    "    INDEX_ENCODING[i] = char\n",
    "\n",
    "print(INDEX_ENCODING)\n",
    "print(CHARACTER_ENCODING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sequence(sequence, deterministic=False):\n",
    "    logits = model(\n",
    "        encode_string(sequence[-INPUT_SEQUENCE_LENGTH:]).unsqueeze(0).to(device)\n",
    "    ) # returns in size (1, INPUT_SEQUENCE_LENGTH, 78)\n",
    "\n",
    "    probabilities = torch.softmax(logits[0], dim=1)\n",
    "\n",
    "    character_indexes = torch.argmax(probabilities) if deterministic else torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    next_sequence = [INDEX_ENCODING[int(index)] for index in character_indexes]\n",
    "    \n",
    "    return next_sequence\n",
    "\n",
    "def get_next_char(sequence, deterministic=False):\n",
    "    logits = model(\n",
    "        encode_string(sequence[-INPUT_SEQUENCE_LENGTH:]).unsqueeze(0).to(device)\n",
    "    ) # returns in size (1, 78)\n",
    "\n",
    "    probabilities = torch.softmax(logits[0], 0)\n",
    "\n",
    "    character_index = torch.argmax(probabilities) if deterministic else torch.multinomial(probabilities, num_samples=1)\n",
    "    \n",
    "    return INDEX_ENCODING[int(character_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n"
     ]
    }
   ],
   "source": [
    "sequence = \"P\"\n",
    "print(get_next_char(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter pushed it awake (but they staring his was at they couldn’t for Peter. “Wendy cried Wendy.\n",
      "\n",
      "You must fit him that the pirate, Wendy, who see of other a lady to three are Nana doubtfully.\n",
      "\n",
      "He was, pleased.\n",
      "\n",
      "No indably, “perhaps join in the street found them. “I thought.\n",
      "\n",
      "Am a moved, and Peter flies followed bititated.\n",
      "\n",
      "“Why, “be glassy-eit it came they flew? Do you noted him at bed. There is one, and again about him.\n",
      "\n",
      "“No, I would do fortudly, “she had a panied blankly at his foe hope you took at frighteen able colours came a hand.\n",
      "\n",
      "“Mermaids!” said Mr. Darling upon the white. She parts the counting-gown, boys would leckety to enter above alighter.\n",
      "\n",
      "Peter knees. There is no doubtedly but there was noted, after cried Cecco she putuer would suddenly stooped himself through hey fellows to adopted,” said Jane whole was gone faithful foe the couldn’t cry the glorying. He chief an hour, come of than the stuck us rejoice of the Azores, “If only Wendy said faithful nursery.\n",
      "\n",
      "“Would have you\n"
     ]
    }
   ],
   "source": [
    "sequence = \"P\"\n",
    "num_generated_characters = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_generated_characters):\n",
    "        sequence += get_next_char(sequence)\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_avaialable_file_name(file_name, extension):\n",
    "    available_file_name = file_name\n",
    "    i = 1\n",
    "    while os.path.isfile(available_file_name + extension):\n",
    "        available_file_name = file_name + f\"_{i}\"\n",
    "        i += 1\n",
    "\n",
    "    return available_file_name + extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    get_avaialable_file_name(\n",
    "        f\"generated_texts/{text_name}_lstm_{INPUT_SEQUENCE_LENGTH}chars\", \".txt\"\n",
    "    ),\n",
    "    \"w\",\n",
    ") as file:\n",
    "    file.write(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    get_avaialable_file_name(\n",
    "        f\"models/{text_name}_gru_{INPUT_SEQUENCE_LENGTH}chars\", \".pth\"\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
